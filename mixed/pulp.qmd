---
title: One Way Anova with a random effect
author: "[Julian Faraway](https://julianfaraway.github.io/)"
date: "`r format(Sys.time(), '%d %B %Y')`"
format: 
  gfm:
    toc: true
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment=NA, 
                      echo = TRUE,
                      fig.path="figs/",
                      dev = 'svglite',  
                      fig.ext = ".svg",
                      warning=FALSE, 
                      message=FALSE)
knitr::opts_knit$set(global.par = TRUE)
```

```{r graphopts, include=FALSE}
par(mgp=c(1.5,0.5,0), mar=c(3.1,3.1,0.1,0), pch=20)
ggplot2::theme_set(ggplot2::theme_bw())
```


See the [introduction](../index.md) for an overview. 

This example is discussed in more detail in my book
[Extending the Linear Model with R](https://julianfaraway.github.io/faraway/ELM/)

Libraries used:

```{r}
library(faraway)
library(ggplot2)
library(lme4)
library(INLA)
library(knitr)
library(rstan, quietly=TRUE)
library(brms)
library(mgcv)
```

# Data

Load up and look at the data, which concerns the brightness of paper
which may vary between operators of the production machinery.

```{r pulpdat}
data(pulp, package="faraway")
summary(pulp)
ggplot(pulp, aes(x=operator, y=bright))+geom_point(position = position_jitter(width=0.1, height=0.0))
```

You can read more about the data by typing `help(pulp)` at the R prompt.

In this example, there are only five replicates per level. There is
no strong reason to reject the normality assumption. We don't care
about the specific operators, who are named a, b, c and d, but we do
want to know how they vary.

# Questions

1. Is there a difference between operators in general?
2. How much is the difference between operators in general?
3. How does the variation between operators compare to the variation
within operators?
4. What is the difference between these four operators?

We are mostly interested in the first three questions.

# Linear model with fixed effects

We start with the simplest analysis although it is not correct. It
will be useful for comparisons. We treat the operator as a fixed effect
meaning that the analysis refers to these four operators and not to
other possible operators. Since we probably don't care about these
particular four operators, this would not be the best choice.

You can use the `lm()` or `aov()` functions:

```{r}
amod = aov(bright ~ operator, pulp)
```

Now test for a difference between operators:

```{r}
anova(amod)
```

We find a statistically significant difference. We can estimate
the coefficients:

```{r}
coef(amod)
```

The treatment coding sets operator a as the reference level. The intercept
is the mean for operator a and the other estimates are differences in the mean
from operator a. We can also test for a difference between pairs of operators:

```{r}
TukeyHSD(amod)
```

Only the d to b difference is found significant.

We have answered the fourth question stated above. We could make some
speculations on the first three questions (what can be said about operators
in general) but our analysis was not designed to do this.

The `aov()` function has been available in R and S before that i.e. at least
30 years. I do not believe it has changed in a long time. It can handle
some simple models but it is has very limited functionality.


# Likelihood inference

We use a model of the form:
$$
y_{ij} = \mu + \alpha_i + \epsilon_{ij} \qquad i=1,\dots ,a
  \qquad j=1,\dots ,n_i,
$$
where the $\alpha_i$ and $\epsilon_{ij}$  are normal
with mean zero, but variances $\sigma_\alpha^2$ and $\sigma^2_\epsilon$,
respectively. 

The default fit uses the REML estimation method:

```{r}
mmod <- lmer(bright ~ 1+(1|operator), pulp)
faraway::sumary(mmod)
```

We see slightly less variation between operators ( $\hat\sigma_a=0.261$ ) than within
operators ( $\sigma_\epsilon=0.326$ ). 

## Hypothesis testing

We can also use the ML method:

```{r}
smod <- lmer(bright ~ 1+(1|operator), pulp, REML = FALSE)
faraway::sumary(smod)
```

The REML method is preferred for estimation but we must use the ML method if we wish
to make hypothesis tests comparing models.

If we want to test for variation between operators, we fit a null model
containing no operator, compute the likelihood ratio statistic and corresponding
p-value:

```{r}
nullmod <- lm(bright ~ 1, pulp)
lrtstat <- as.numeric(2*(logLik(smod)-logLik(nullmod)))
pvalue <- pchisq(lrtstat,1,lower=FALSE)
data.frame(lrtstat, pvalue)
```

Superficially, the p-value greater than 0.05 suggests no strong evidence
against that hypothesis that there is no variation among the operators. But
there is good reason to doubt the accuracy of the standard approximation of the chi-squared null distribution when
testing a parameter on the boundary of the space (as we do here at zero). A
parametric bootstrap can be used where we generate samples from the null
and compute the test statistic repeatedly:

```{r pulpparaboot, cache=TRUE}
lrstat <- numeric(1000)
set.seed(123)
for(i in 1:1000){
   y <- unlist(simulate(nullmod))
   bnull <- lm(y ~ 1)
   balt <- lmer(y ~ 1 + (1|operator), pulp, REML=FALSE)
   lrstat[i] <- as.numeric(2*(logLik(balt)-logLik(bnull)))
  }
```

Check the proportion of simulated test statistics that are close to zero:

```{r}
mean(lrstat < 0.00001)
```

Clearly, the test statistic does not have a chi-squared distribution under
the null. We can compute the proportion that exceed the observed test
statistic of 2.5684:

```{r}
mean(lrstat > 2.5684)
```

This is a more reliable p-value for our hypothesis test which suggest there
is good reason to reject the null hypothesis of no variation between operators.

More sophisticated methods of inference are discussed in 
[Extending the Linear Model with R](https://julianfaraway.github.io/faraway/ELM/)

## Confidence intervals

We can use bootstrap again to compute confidence intervals for
the parameters of interest:

```{r pulpboot, cache=TRUE}
confint(mmod, method="boot")
```

We see that the lower end of the confidence interval for the operator SD
extends to zero.

## Random effects

Even though we are most interested in the variation between operators,
we can still estimate their individual effects:

```{r}
ranef(mmod)$operator
```

Approximate 95% confidence intervals can be displayed with:

```{r pulpebar}
dd = as.data.frame(ranef(mmod))
ggplot(dd, aes(y=grp,x=condval)) +
        geom_point() +
        geom_errorbarh(aes(xmin=condval -2*condsd,
                           xmax=condval +2*condsd), height=0)
```

# INLA

Integrated nested Laplace approximation is a method of Bayesian computation
which uses approximation rather than simulation. More can be found
on this topic in [Bayesian Regression Modeling with INLA](http://julianfaraway.github.io/brinla/) and the 
[chapter on GLMMs](https://julianfaraway.github.io/brinlabook/chaglmm.html)

At the time of writing, INLA is undergoing some changes that
have some impact on the fitting of mixed effect models. Previously,
the model fitting proceeded in two stages - the first step computed
the posteriors for most of the parameters while a second step makes
a refinement to compute posteriors for the hyperparameters (the variances
of the random terms in this case). This is termed *classic* mode. Now
the idea to compute all the posteriors in one stage called *experimental*
mode below. We would have preferred to stick with classic mode for
continuity reasons but this mode is no longer functional in the most
recent version of INLA. You can see an [older analyis](http://julianfaraway.github.io/brinla/examples/oneway.html) of
the same data using the previous methodology.
We use the most recent computational methodology
and also opt for a shorter output summary:


```{r}
inla.setOption(inla.mode="experimental")
inla.setOption("short.summary",TRUE)
```

Run the INLA model with default priors:

```{r pulpinladefault, cache=TRUE}
imod <- inla(bright ~ f(operator, model="iid"),
             family="gaussian",
             data=pulp)
```

The summary of the posterior distribution for the fixed effects (which is only the intercept in this example):

```{r}
imod$summary.fixed |> kable()
```

The posterior mean is the same as the (RE)ML estimate. The posterior distribution
of the hyperparameters (precision of the error and operator terms)


```{r}
imod$summary.hyperpar |> kable()
```
Precision for the operator term is unreasonably high. This implies
a strong belief that there is no variation between the operators which we would find hard to believe. This is due to the default diffuse gamma prior on the precisions
which put almost all the weight on the error variation and not nearly enough on
the operator variation. We need to change the prior.

## Halfnormal prior on the SDs

We try a halfnormal prior with low precision instead. A precision of 0.01 corresponds to an SD of 10. (It is possible to vary the mean but we have set this to 
zero to achieve a halfnormal distribution). 
This is substantially larger than the SD of the response so the information supplied is very weak.

```{r pulpinlatn, cache=TRUE}
tnprior <- list(prec = list(prior="logtnormal", param = c(0,0.01)))
imod <- inla(bright ~ f(operator, model="iid", hyper = tnprior),
               family="gaussian", 
               data=pulp)
summary(imod)
```

The results appear more plausible. Transform to the SD scale 


```{r}
sigmaalpha <- inla.tmarginal(function(x)1/sqrt(exp(x)),
                             imod$internal.marginals.hyperpar[[2]])
sigmaepsilon <- inla.tmarginal(function(x)1/sqrt(exp(x)),
                                imod$internal.marginals.hyperpar[[1]])
```

and output the summary statistics (note that transforming the summary
statistics on the precision scale only works for the quantiles)


```{r}
sigalpha = c(inla.zmarginal(sigmaalpha, silent = TRUE),
            mode=inla.mmarginal(sigmaalpha))
sigepsilon = c(inla.zmarginal(sigmaepsilon, silent = TRUE),
              mode=inla.mmarginal(sigmaepsilon))
rbind(sigalpha, sigepsilon) 
```

The posterior mode is most comparable with the (RE)ML estimates computed
above. In this respect, the results are similar.

We can also get summary statistics on the random effects:

```{r}
imod$summary.random$operator |> kable()
```

Plot the posterior densities for the two SD terms:

```{r plotsdspulp}
ddf <- data.frame(rbind(sigmaalpha,sigmaepsilon),
                  errterm=gl(2,dim(sigmaalpha)[1],labels = c("alpha","epsilon")))
ggplot(ddf, aes(x,y, linetype=errterm))+
  geom_line()+xlab("bright")+ylab("density")+xlim(0,2)
```

We see that the operator SD less precisely known than the error SD. Although
the mode for the operator is smaller, there is a substantial chance it could
be much higher than the error SD.

Is there any variation between operators? We framed this question as an
hypothesis test previously but that is not sensible in this framework. We might
ask the probability that the operator SD is zero. Since we have posited a
continuous prior that places no discrete mass on zero, the posterior probability
will be zero, regardless of the data. Instead we might ask the probability 
that the operator SD is small. Given the response is measured to one decimal
place, 0.1 is a reasonable representation of *small* if we take this to mean
the smallest amount we care about.

We can compute the probability that the operator SD is smaller than 0.1:

```{r}
inla.pmarginal(0.1, sigmaalpha)
```

The probability is small but not entirely negligible.


## Informative gamma priors on the precisions

Now try more informative gamma priors for the precisions. Define it so the mean value of gamma prior is set to the inverse of the
variance of the response. We expect the two error variances to be lower than the response variance so this is an overestimate.
The variance of the gamma prior (for the precision) is controlled by the `apar` shape parameter in the code. `apar=1` is the
exponential distribution. Shape values less than one result in densities that have a mode at zero and decrease monotonely. These
have greater variance and hence less informative.

```{r pulpinlaig, cache=TRUE}
apar <- 0.5
bpar <- var(pulp$bright)*apar
lgprior <- list(prec = list(prior="loggamma", param = c(apar,bpar)))
imod <- inla(bright ~ f(operator, model="iid", hyper = lgprior),
               family="gaussian", 
               data=pulp)
summary(imod)
```

Compute the summaries as before:

```{r}
sigmaalpha <- inla.tmarginal(function(x)1/sqrt(exp(x)),
                             imod$internal.marginals.hyperpar[[2]])
sigmaepsilon <- inla.tmarginal(function(x)1/sqrt(exp(x)),
                            imod$internal.marginals.hyperpar[[1]])
sigalpha = c(inla.zmarginal(sigmaalpha, silent = TRUE),
            mode=inla.mmarginal(sigmaalpha))
sigepsilon = c(inla.zmarginal(sigmaepsilon, silent = TRUE),
              mode=inla.mmarginal(sigmaepsilon))
rbind(sigalpha, sigepsilon) 
```

Slightly different outcome.


Make the plots:
```{r plotsdspulpig}
ddf <- data.frame(rbind(sigmaalpha,sigmaepsilon),
                  errterm=gl(2,dim(sigmaalpha)[1],labels = c("alpha","epsilon")))
ggplot(ddf, aes(x,y, linetype=errterm))+
  geom_line()+xlab("bright")+ylab("density")+xlim(0,2)
```


The posterior for the error SD is quite similar to that seen previously but the operator SD is larger and
bounded away from zero and less dispersed.

We can compute the probability that the operator SD is smaller than 0.1:

```{r}
inla.pmarginal(0.1, sigmaalpha)
```

The probability is very small. The choice of prior may be unsuitable in that no density is placed on
an SD=0 (or infinite precision). We also have very little prior weight on low SD/high precision values. This
leads to a posterior for the operator with very little density assigned to small values of the SD. But we
can see from looking at the data or from prior analyses of the data that there is some possibility that the
operator SD is very small.

## Penalized Complexity Prior

In [Simpson (2017)](https://doi.org/10.1214/16-STS576), penalized complexity priors are proposed. This
requires that we specify a scaling for the SDs of the random effects. We use the SD of the residuals
of the fixed effects only model (what might be called the base model in the paper) to provide this scaling.

```{r pulpinlapc, cache=TRUE}
sdres <- sd(pulp$bright)
pcprior <- list(prec = list(prior="pc.prec", param = c(3*sdres,0.01)))
imod <- inla(bright ~ f(operator, model="iid", hyper = pcprior),
               family="gaussian", 
               data=pulp)
summary(imod)
```

Compute the summaries as before:

```{r}
sigmaalpha <- inla.tmarginal(function(x)1/sqrt(exp(x)),
                             imod$internal.marginals.hyperpar[[2]])
sigmaepsilon <- inla.tmarginal(function(x)1/sqrt(exp(x)),
                            imod$internal.marginals.hyperpar[[1]])
sigalpha = c(inla.zmarginal(sigmaalpha, silent = TRUE),
            mode=inla.mmarginal(sigmaalpha))
sigepsilon = c(inla.zmarginal(sigmaepsilon, silent = TRUE),
              mode=inla.mmarginal(sigmaepsilon))
rbind(sigalpha, sigepsilon) 
```

We get a similar result to the truncated normal prior used earlier although the operator SD is generally smaller.

Make the plots:
```{r plotsdspulppc}
ddf <- data.frame(rbind(sigmaalpha,sigmaepsilon),
                  errterm=gl(2,dim(sigmaalpha)[1],labels = c("alpha","epsilon")))
ggplot(ddf, aes(x,y, linetype=errterm))+
  geom_line()+xlab("bright")+ylab("density")+xlim(0,2)
```

We can compute the probability that the operator SD is smaller than 0.1:

```{r}
inla.pmarginal(0.1, sigmaalpha)
```

The probability is small but not insubstantial.

We can plot the posterior density of $\mu$ along with a 95% credibility interval:

```{r pulpmargfix}
mu <- data.frame(imod$marginals.fixed[[1]])
cbound = inla.qmarginal(c(0.025,0.975),mu)
ggplot(mu, aes(x,y)) + geom_line() + 
  geom_vline(xintercept = cbound) +
  xlab("brightness")+ylab("density")
```

We can plot the posterior marginals of the random effects:

```{r pulprandeffpden}
nlevels = length(unique(pulp$operator))
rdf = data.frame(do.call(rbind,imod$marginals.random$operator))
rdf$operator = gl(nlevels,nrow(rdf)/nlevels,labels=1:nlevels)
ggplot(rdf,aes(x=x,y=y,group=operator, color=operator)) + 
  geom_line() +
  xlab("") + ylab("Density")
```

We see that operators 1 and 2 tend to be lower than 3 and 4. There
is substantial overlap so we would hesitate to declare any difference
between a pair of operators.

# STAN

[STAN](https://mc-stan.org/) performs Bayesian inference using
MCMC.

Set up STAN to use multiple cores. Set the random number seed for reproducibility.

```{r}
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
set.seed(123)
```

We need a STAN command file `pulp.stan` which we view here:

```{r}
writeLines(readLines("../stancode/pulp.stan"))
```

We have used
uninformative priors for the overall mean and the two variances.
Prepare data in a format consistent with the command file. Needs to be
a list. Can't use the word `operator` since this is reserved for
system use in STAN.

```{r}
pulpdat <- list(N=nrow(pulp),
                J=length(unique(pulp$operator)),
                response=pulp$bright,
                predictor=as.numeric(pulp$operator))
```

Break the fitting process into three steps:

```{r pulpstancomp, cache=TRUE}
rt <- stanc(file="../stancode/pulp.stan")
suppressMessages(sm <- stan_model(stanc_ret = rt, verbose=FALSE))
system.time(fit <- sampling(sm, data=pulpdat))
```

By default, we use 2000 iterations but repeated with independent
starts 4 times giving 4 chains. We can thin but do not by default. The
warmup period is half the number of observations (which is very
conservative in this instance).

We get warning messages about the fit. Since the default number
of 2000 iterations runs in seconds, we can simply run a lot
more iterations. This is rather lazy and would not be viable
for more expensive computations, but sometimes CPU effort is
preferred to mental effort.

```{r pulpstanbigiter, cache=TRUE}
system.time(fit <- sampling(sm, data=pulpdat, iter=100000))
```

The same underlying problems remain but the inference will now
be more reliable.

## Diagnostics

Diagnostics to check the convergence are worthwhile. We plot
the sampled $\mu$ in the four chains, choosing only every 100th
observation (the plot becomes very dense if we show everything). The warm-up period is excluded. 

```{r pulpmudiag}
pname <- "mu"
muc <- rstan::extract(fit, pars=pname,  permuted=FALSE, inc_warmup=FALSE)
mdf <- reshape2::melt(muc)
mdf |> dplyr::filter(iterations %% 100 == 0) |> 
ggplot(aes(x=iterations,y=value,color=chains)) + geom_line() + ylab(mdf$parameters[1])
```

We see the traces of the four chains overlaid in different colors. The chains appear roughly stationary although there are some occasional larger 
excursions (which is why we needed more iterations).

The similar plots can be produced for the two variance terms although note that STAN uses the standard deviations (which we also prefer). Here is the group (operator) SD:

```{r pulpalphadiag}
pname <- "sigmaalpha"
muc <- rstan::extract(fit, pars=pname,  permuted=FALSE, inc_warmup=FALSE)
mdf <- reshape2::melt(muc)
mdf |> dplyr::filter(iterations %% 100 == 0) |> 
  ggplot(aes(x=iterations,y=value,color=chains)) + 
  geom_line() + ylab(mdf$parameters[1])
```

This looks acceptable. We expect that the distribution will be asymmetric so this is no concern. The chains stay away
from zero (or close to it). Here's the same plot for the error SD.

```{r pulpepsdiag}
pname <- "sigmaepsilon"
muc <- rstan::extract(fit, pars=pname,  permuted=FALSE, inc_warmup=FALSE)
mdf <- reshape2::melt(muc)
mdf |> dplyr::filter(iterations %% 100 == 0) |> 
  ggplot(aes(x=iterations,y=value,color=chains)) + 
  geom_line() + ylab(mdf$parameters[1])
```

Again this looks satisfactory.

## Output summaries

We consider only the parameters of immediate interest:

```{r}
print(fit, pars=c("mu","sigmaalpha","sigmaepsilon","a"))
```

We see the posterior mean, SE and SD of the samples. 
We see some quantiles from which we could construct a 95% credible
interval (for example). 
The `n_eff` is a rough measure of the sample size taking into account 
the correlation in the
samples. The effective sample sizes for the mean and operator SD primary parameters is not large (considering the number of iterations) although adequate enough for most purposes. The Rhat statistic is known as the
potential scale reduction factor. Values much greater than one indicate that additional samples would significantly improve
the inference. In this case, the factors are all one so we feel no inclination to draw more samples.

We can also get the posterior means alone.

```{r}
(get_posterior_mean(fit, pars=c("mu","sigmaalpha","sigmaepsilon","a")))
```

We see that we get this information for each chain as well as overall. This gives a sense of why running more
than one chain might be helpful in assessing the uncertainty in the posterior inference.



## Posterior Distributions

We can use `extract` to get at various components of the STAN fit. We plot the posterior densities for the SDs:

```{r pulppdae}
postsig <- rstan::extract(fit, pars=c("sigmaalpha","sigmaepsilon"))
ref <- reshape2::melt(postsig,value.name="bright")
ggplot(ref,aes(x=bright, color=L1))+
  geom_density()+
  xlim(0,2) +
  guides(color=guide_legend(title="SD"))
```

We see that the error SD can be localized much more than the operator SD.
We can also look at the operator random effects:

```{r pulpstanre}
opre <- rstan::extract(fit, pars="a")
ref <- reshape2::melt(opre, value.name="bright")
ref[,2] <- (LETTERS[1:4])[ref[,2]]
ggplot(data=ref,aes(x=bright, color=Var2))+geom_density()+guides(color=guide_legend(title="operator"))
```

We see that the four operator distributions overlap.

## Tail probability

Previously, we took an interest in whether there is any variation
between operators and answered this question with a computation
of the probability that the operator SD is less than 0.1. We
computed the proportion of sampled values less than 0.1.

```{r}
muc <- rstan::extract(fit, pars="sigmaalpha",  permuted=FALSE, inc_warmup=FALSE)
mdf <- reshape2::melt(muc)
mean(mdf$value < 0.1)
```

This is a somewhat larger probability than seen previously. The
value obtained is sensitive to the choice of prior on the error SD.
This can be changed within STAN but it is easier to experiment
with this using BRMS.

# BRMS

[BRMS](https://paul-buerkner.github.io/brms/) stands for Bayesian Regression Models with STAN. It provides
a convenient wrapper to STAN functionality.

Fitting the model is very similar to `lmer` as seen above:

```{r brmfit, cache=TRUE}
suppressMessages(bmod <- brm(bright ~ 1+(1|operator), pulp))
```

We get some warnings but not as severe as seen with our STAN fit
above. We can obtain some posterior densities and diagnostics with:

```{r pulpbrmsdiag}
plot(bmod)
```

We can look at the STAN code that `brms` used with:

```{r}
stancode(bmod)
```

We see that `brms` is using student t distributions with 3 degrees of
freedom for the priors. For the two error SDs, this will be truncated at
zero to form half-t distributions. You can get a more explicit description
of the priors with `prior_summary(bmod)`. These are qualitatively similar to the
half-normal and the PC prior used in the INLA fit. This explains why
we encountered fewer problems in the fit because we are supplying
more informative priors. Nevertheless, we do need to increase the
number of iterations for more accurate estimation of tail probabilities.

```{r pulpbrmsmore, cache=TRUE}
bmod <- brm(bright ~ 1+(1|operator), pulp, iter=10000, cores = 4)
```

Because the STAN programme was compiled earlier, this takes much less time
overall even though we are doing 5 times as many iterations as the default
number of 2000. We examine the fit:

```{r}
summary(bmod)
```

We now have better effective sample sizes. We can estimate
the tail probability as before

```{r}
bps = posterior_samples(bmod)
mean(bps$sd_operator__Intercept < 0.1)
```

A somewhat higher value than seen previously. The priors used here put
greater weight on smaller values of the SD.

# MGCV

It is possible to fit some GLMMs within the GAM framework of the `mgcv`
package. An explanation of this can be found in this 
[blog](https://fromthebottomoftheheap.net/2021/02/02/random-effects-in-gams/)
and also in this [blog](https://www.tjmahr.com/random-effects-penalized-splines-same-thing/)

The `operator` term must be a factor for this to work:

```{r}
gmod = gam(bright ~ 1 + s(operator, bs = 're'), data=pulp, method="REML")
```

and look at the summary output:

```{r}
summary(gmod)
```

We get the estimate and SE for the fixed effect (intercept in this example).
We also get a test on the random effect (as described in this [article](https://doi.org/10.1093/biomet/ast038). The hypothesis of no variation
between the operators is rejected.

We can get an estimate of the operator and error SD:

```{r}
gam.vcomp(gmod)
```

which is the same as the REML estimate from `lmer` earlier.

The random effect estimates for the four operators can be found with:

```{r}
coef(gmod)
```

which is again the same as before.

## GINLA

In [Wood (2019)](https://doi.org/10.1093/biomet/asz044), a
simplified version of INLA is proposed. The first
construct the GAM model without fitting and then use
the `ginla()` function to perform the computation.

```{r}
gmod = gam(bright ~ 1 + s(operator, bs = 're'), data=pulp, fit = FALSE)
gimod = ginla(gmod)
```

We get the posterior density for the intercept as:

```{r pulpginlaint}
plot(gimod$beta[1,],gimod$density[1,],type="l",xlab="brightness",ylab="density")
```

and for the fixed effects as:

```{r pulpginlareff}
plot(gimod$beta[2,],gimod$density[2,],type="l",xlim=c(-0.8,0.6),
     xlab="brightness",ylab="density")
for(i in 3:5){
  lines(gimod$beta[i,],gimod$density[i,],lty=i-1)
}
```

It is not straightforward to obtain the posterior densities of
the hyperparameters. 

# Discussion

It is interesting to compare these methods. We cannot make numerical
comparisons across the whole set because the models, assumptions and
even statistical philosophies are not the same. We cannot hope to claim
one method is objectively better than another. Even so, the researcher,
who wants to know the nature of the difference between the operators, must
decide between them and so a qualitative comparison is worthwhile.

Let us assume the perspective of researcher who is not statistician. Although
such researchers are usually tolerant of complexity and ambiguity in research
questions, they are far less willing to wade into theoretical and
computational marshes of statistics. They would like to submit a manuscript
for publication where questions won't be raised over their choice of
statistical methodology. They want to software that is easy to use and does
not require opaque, complex and finicky computations.

> All the software discussed here has been provided free. Researchers have
gone beyond the call of duty to provide this. Academic research articles
require only text and perhaps some demonstration of software. The production
of functional software requires immensely more effort and is usually
not rewarded in a career or financial sense. The efforts of these producers
has advanced science immeasurably beyond what would have been achieved by
purely commercial statistical software. Any criticism below should be
measured against these huge achievements.

1. The fixed effect linear model solution is by far the easiest to implement.
The methodology has been established for decades and the manner in which
such results should be presented is well understood and accepted. It requires
only the installation of base R and runs almost instantly. The only problem is that it does not properly answer the main questions of interest.

2. The likelihood-based analysis using `lme4` is relatively straightforward. We
need to install only one well-established R package: `lme4`. This package has been
used and installed so many times that we are unlikely to encounter any unexpected
problems. We do need to understand the difference between ML and REML estimation.
Testing for the operator effect is more complicated than we might like. 

3. Many years ago, `lme4` would produce a likelihood ratio test that was not correct for
this class of models (more precisely, the null distribution was wrong). Some statisticians might have had reservations about the use of such tests but for a long time, they were routinely used in these 
circumstances. After all, there are many tests used now which we know to be
approximations but feel comfortable with accepting. No one wants to open that can of worms. But eventually the
evidence built up against these particular tests and the functionality was
abruptly withdrawn from `lme4`.  Having used these tests in the first edition
of *Extending the Linear Models with R*, I was perturbed. I had recommended
a procedure that was wrong. Many others who had published results were also
discomfited. But getting it right is more important than any embarassment. The
episode illustrated the uncomfortable fact that not all statistical procedures
can be regarded as incontrovertibly correct for all time.

4. I used a parametric bootstrap procedure here which requires some undesirable
complication to use from the perspective of the non-statistician. There
are R packages which provide other good tests but also require more
advanced understanding. All this is a nuisance for the non-statistician writing
up results for publication in a non-statistics journal. The referees may
well not be familiar with these tests and trouble may result. The
fixed effects analysis would not be questioned in this way.

5. The Bayesian approaches introduce additional aspects of complexity
into the analysis. Our non-statistician researcher will have to decide
whether to brave the possible complications of submitting a Bayesian analysis
to an academic journal. It's fine for me to extol the virtues of a Bayesian
analysis but no good if the article gets rejected as a result. Putting
that aside, we must specify priors. In some cases, the priors do not make 
much difference to the conclusions. In this example, the priors make a large
difference to the chance that the operator variation is negligible. This
reflects the small size of the data and that we have information on only
four operators. Although this is the reality, it does contrast with the 
definitive answers provided by the previous analyses.

6. The Bayesian approaches also entail practical and computational complications.
Installing INLA or STAN involves various difficulties that may or may not arise
depending on the operating system and other software on your computer. If I were
to ask a class of 100 undergraduates to get INLA or STAN up and running, I would
consign myself to many hours of dealing with frustrated emails involving computer
problems that I have only the vaguest ideas about solving. It would be 
more practical to ask them to use a central server where everything is installed 
but this results in other problems regarding access and usage. In this example,
STAN outputs a slew of obscure compiler warnings and even some errors. Some
googling reveals that some other people receive much the same set of warnings
as me. But many more do not because otherwise the developers would have fixed it.
The errors and warnings are probably due to the specific configuration of
my computer. I never did discover the source of the problem - suggestions
on the internet required uninstalling and reinstalling large pieces of software. 
Fortunately, I could see that the errors and warnings could be ignored.
After several decades of fiddling with computers to get software to work, I (mostly) possess the patience and experience to deal with this sort of problem. That's far from true for many potential users. 

7. Both STAN and INLA are being actively developed. Of course, it's good to know
that functionality and performance are being improved. In both cases, I installed
the release version and encountered problems. I fared better with the development
versions although in the case of INLA, I had to use an "experimental" mode. It's
not unusual to complete an analysis, come back to it a year later and find
that the results are different in some important way or that the code does not
run at all. Again I can ride along with this but less experienced users find
this problematic. At some point, they may ask "How do I know if this is the
right answer?" and the reassurance is not entirely convincing. In contrast,
it is not unusual to feed S vintage code into base R and have it give the same
results as it did 30 years ago.

8. Specifying and fitting Bayesian models is inherently more complicated than
standard linear models or those fit by `lme4`. We expect to work harder but
surely there is some scope for a notational advance in model specification.
The extended Wilkinson-Rogers model notation and the grammar of graphics ideas
seen in `ggplot2` are examples where notation has helped advance understanding
beyond simple convenience. STAN requires us to learn a new language merely
to specify the model. Fortunately, `brms` (and `rstanarm`) allow less advanced users
to skip these complications.

9. Fitting Bayesian models is more likely to go wrong than GLMM models. For
our STAN model, there were insufficient iterations. We did get a warning
that suggested the solution of more iterations. Some warnings remained but
we knew that is was safe to ignore them. With the INLA model, the default
prior led to some unbelievable results. It took some knowledge to know
the solution was to use a more informative prior (and this would have
been another solution to the STAN fitting problem.) There were less problems
in using `lme4` although the bootstrapping does throw large numbers of warnings
when the parameter estimate falls on the boundary. Again we had to
know it was safe to let this pass. All this requires some expertise and
may confuse the non-statistician.


# Package version info

These analyses required a shockingly large number of packages. One worries
that a small change in just one of these packages might cause the analysis above
to fail or change in some unexpected manner. Not all of the attached
packages are actually used but it is hard to know what we could do without.


```{r}
sessionInfo()
```
