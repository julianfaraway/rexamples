---
title: Split Plot Design
author: "[Julian Faraway](https://julianfaraway.github.io/)"
date: "`r format(Sys.time(), '%d %B %Y')`"
format: 
  gfm:
    toc: true
---

```{r}
#| label: global_options
#| include: false
knitr::opts_chunk$set(comment=NA, 
                      echo = TRUE,
                      fig.path="figs/",
                      dev = 'svglite',  
                      fig.ext = ".svg",
                      warning=FALSE, 
                      message=FALSE)
knitr::opts_knit$set(global.par = TRUE)
```

```{r}
#| label: graphopts
#| include: false
par(mgp=c(1.5,0.5,0), mar=c(3.1,3.1,0.1,0), pch=20)
ggplot2::theme_set(ggplot2::theme_bw())
```

See the [introduction](index.md) for an overview. 

This example is discussed in more detail in my book
[Extending the Linear Model with R](https://julianfaraway.github.io/faraway/ELM/)

Required libraries:

```{r}
library(faraway)
library(ggplot2)
library(lme4)
library(pbkrtest)
library(RLRsim)
library(INLA)
library(knitr)
library(cmdstanr)
register_knitr_engine(override = FALSE)
library(brms)
library(mgcv)
```

# Data

In an agricultural field trial, the
objective was to determine the effects of two crop varieties and four
different irrigation methods. Eight fields were available, but only one
type of irrigation may be applied to each field. The fields may be
divided into two parts with a different variety planted in each half.
The whole plot factor is the method of irrigation, which should be
randomly assigned to the fields. Within each field, the variety is
randomly assigned.

Load in and plot the data:

```{r}
#| label: irriplot
data(irrigation, package="faraway")
summary(irrigation)
ggplot(irrigation, aes(y=yield, x=field, shape=variety, color=irrigation)) + geom_point()
```

# Mixed Effect Model

The irrigation and variety are fixed effects, but the field is 
a random effect. We must also consider the interaction between field
and variety, which is necessarily also a random effect because one of
the two components is random. The fullest model that we might consider
is: 

$$y_{ijk} = \mu + i_i + v_j + (iv)_{ij} + f_k + (vf)_{jk} + \epsilon_{ijk}$$

where $\mu, i_i, v_j, (iv)_{ij}$ are fixed effects; the rest are random
having variances $\sigma^2_f$, $\sigma^2_{vf}$ and $\sigma^2_\epsilon$.
Note that we have no $(if)_{ik}$ term in this model. It would not be
possible to estimate such an effect since only one type of irrigation
is used on a given field; the factors are not crossed. Unfortunately,
it is not possible to
distinguish the variety within the field variation.  We would need
more than one observation per variety within each field for us to
separate the two variabilities. We resort to a simpler model that omits the variety by field interaction random effect: 

$$y_{ijk} = \mu + i_i + v_j + (iv)_{ij} + f_k +  \epsilon_{ijk}$$


We fit this model with:

```{r}
lmod <- lme4::lmer(yield ~ irrigation * variety + (1|field), irrigation)
faraway::sumary(lmod)
```

The fixed effects don't look very significant. We could use a parametric
bootstrap to test this but it's less work to use the `pbkrtest` package
which implements the Kenward-Roger approximation. First test the interaction:

```{r}
lmoda <- lmer(yield ~ irrigation + variety + (1|field),data=irrigation)
faraway::sumary(lmoda)
pbkrtest::KRmodcomp(lmod, lmoda)
```

We can drop the interaction. Now test for a variety effect:

```{r}
lmodi <- lmer(yield ~ irrigation + (1|field), irrigation)
KRmodcomp(lmoda, lmodi)
```

The variety can go also. Now check the irrigation method.

```{r}
lmodv <- lmer(yield ~  variety + (1|field), irrigation)
KRmodcomp(lmoda, lmodv)
```

This can go also. As a final check, lets compare the null model
with no fixed effects to the full model.

```{r}
lmodn <- lmer(yield ~  1 + (1|field), irrigation)
KRmodcomp(lmod, lmodn)
```

This confirms the lack of statistical significance for the variety
and irrigation factors.


We can check the significance of the random effect (field) term with:

```{r}
RLRsim::exactRLRT(lmod)
```

We can see that there is a significant variation among the fields.

# INLA

Integrated nested Laplace approximation is a method of Bayesian computation
which uses approximation rather than simulation. More can be found
on this topic in [Bayesian Regression Modeling with INLA](http://julianfaraway.github.io/brinla/) and the 
[chapter on GLMMs](https://julianfaraway.github.io/brinlabook/chaglmm.html)

Use the most recent computational methodology:


```{r}
inla.setOption(inla.mode="experimental")
inla.setOption("short.summary",TRUE)
```

Try the default INLA fit

```{r}
#| label: irriinladef
#| cache: true
formula <- yield ~ irrigation + variety +f(field, model="iid")
result <- inla(formula, family="gaussian", data=irrigation)
summary(result)
```

Default looks more plausible than [one way](pulp.md) and [RBD](penicillin.md) examples.

Compute the transforms to an SD scale for the field and error. Make a table of summary statistics for the posteriors:

```{r}
#| label: sumstats
sigmaalpha <- inla.tmarginal(function(x) 1/sqrt(exp(x)),result$internal.marginals.hyperpar[[2]])
sigmaepsilon <- inla.tmarginal(function(x) 1/sqrt(exp(x)),result$internal.marginals.hyperpar[[1]])
restab=sapply(result$marginals.fixed, function(x) inla.zmarginal(x,silent=TRUE))
restab=cbind(restab, inla.zmarginal(sigmaalpha,silent=TRUE))
restab=cbind(restab, inla.zmarginal(sigmaepsilon,silent=TRUE))
colnames(restab) = c("mu","ir2","ir3","ir4","v2","alpha","epsilon")
data.frame(restab)
```

Also construct a plot the SD posteriors:

```{r}
#| label: plotsdsirri
ddf <- data.frame(rbind(sigmaalpha,sigmaepsilon),errterm=gl(2,nrow(sigmaalpha),labels = c("alpha","epsilon")))
ggplot(ddf, aes(x,y, linetype=errterm))+geom_line()+xlab("yield")+ylab("density")+xlim(0,10)
```

Posteriors look OK.

## Informative Gamma priors on the precisions

Now try more informative gamma priors for the precisions. Define it so
the mean value of gamma prior is set to the inverse of the variance of
the residuals of the fixed-effects only model. We expect the two error
variances to be lower than this variance so this is an overestimate.
The variance of the gamma prior (for the precision) is controlled by
the `apar` shape parameter.

```{r}
#| label: irriinlaig
#| cache: true
apar <- 0.5
lmod <- lm(yield ~ irrigation+variety, data=irrigation)
bpar <- apar*var(residuals(lmod))
lgprior <- list(prec = list(prior="loggamma", param = c(apar,bpar)))
formula = yield ~ irrigation+variety+f(field, model="iid", hyper = lgprior)
result <- inla(formula, family="gaussian", data=irrigation)
summary(result)
```

Compute the summaries as before:

```{r}
#| ref.label: sumstats
```

Make the plots:

```{r}
#| label: irrigam
#| ref.label: plotsdsirri
```

Posteriors look OK.

## Penalized Complexity Prior

In [Simpson et al (2015)](http://arxiv.org/abs/1403.4630v3), penalized complexity priors are proposed. This
requires that we specify a scaling for the SDs of the random effects. We use the SD of the residuals
of the fixed effects only model (what might be called the base model in the paper) to provide this scaling.

```{r}
#| label: irriinlapc
#| cache: true
lmod <- lm(yield ~ irrigation+variety, irrigation)
sdres <- sd(residuals(lmod))
pcprior <- list(prec = list(prior="pc.prec", param = c(3*sdres,0.01)))
formula <- yield ~ irrigation+variety+f(field, model="iid", hyper = pcprior)
result <- inla(formula, family="gaussian", data=irrigation)
summary(result)
```

Compute the summaries as before:

```{r}
#| ref.label: sumstats
```

Make the plots:

```{r}
#| label: irripc
#| ref.label: plotsdsirri
```

Posteriors look OK. Not much difference between the three priors tried here.

# STAN

[STAN](https://mc-stan.org/) performs Bayesian inference using
MCMC.

I use `cmdstanr` to access Stan from R.

You see below the Stan code to fit our model. Rmarkdown allows the use
of Stan chunks (elsewhere I have R chunks). The chunk header looks
like this. 

STAN chunk will be compiled to 'mod'. Chunk header is:
```
cmdstan, output.var="mod", override = FALSE
```

```{cmdstan, output.var="mod", override = FALSE}
data {
  int<lower=0> N;
  array[N] int<lower=1,upper=8> field;
  array[N] int<lower=1,upper=4> irrigation;
  array[N] int<lower=1,upper=2> variety;
  array[N] real y;
}
transformed data { // need to manually create dummy variables
  vector[N] irmeth2;
  vector[N] irmeth3;
  vector[N] irmeth4;
  vector[N] var2;
  for (i in 1:N) {
    irmeth2[i] = irrigation[i] == 2;
    irmeth3[i] = irrigation[i] == 3;
    irmeth4[i] = irrigation[i] == 4;
    var2[i] = variety[i] == 2;
  }
}
parameters {
  vector[8] eta;
  real mu;
  real ir2;
  real ir3;
  real ir4;
  real va2;
  real<lower=0> sigmaf;
  real<lower=0> sigmay;
}
transformed parameters {
  vector[8] fld;
  vector[N] yhat;

  fld = sigmaf * eta;

  for (i in 1:N)
    yhat[i] = mu+ir2*irmeth2[i]+ir3*irmeth3[i]+ir4*irmeth4[i]+va2*var2[i]+fld[field[i]];

}
model {
  eta ~ normal(0, 1);

  y ~ normal(yhat, sigmay);
}
```


We have used uninformative priors
for the fixed effects and the two variances. Prepare data in a format consistent with the command file. Needs to be a list.

```{r}
irridat <- with(irrigation,list(N=length(yield), y=yield, field=as.numeric(field), irrigation=as.numeric(irrigation), variety=as.numeric(variety)))
```

Do the MCMC sampling:

```{r}
fit <- mod$sample(
  data = irridat, 
  seed = 123, 
  chains = 4, 
  parallel_chains = 4,
  refresh = 500 # print update every 500 iters
)
```





## Diagnostics

Extract the draws into a convenient dataframe format:

```{r}
draws_df <- fit$draws(format = "df")
```


For the field SD:

```{r}
#| label: irristansigmaf
ggplot(draws_df,
       aes(x=.iteration,y=sigmaf,color=factor(.chain))) + geom_line() +
  labs(color = 'Chain', x="Iteration")
```

which also looks reasonable.


## Output summaries

Examine the output for the parameters we are mostly interested in:

```{r}
fit$summary(c("mu","ir2","ir3","ir4","va2","sigmaf","sigmay","fld"))
```

We see the posterior mean, median and SD, MAD of the samples. We see some quantiles from which we could construct a 95% credible
interval (for example).  The effective sample sizes for the primary parameters is good enough for most purposes.  The $\hat R$ statistics are good.

Notice that the posterior mean for field SD is substantially larger
than seen in the mixed effect model or the previous INLA models.

## Posterior Distributions

Plot the posteriors for the variance components

```{r}
#| label: irristanvc
sdf = stack(draws_df[,startsWith(colnames(draws_df),"sigma")])
colnames(sdf) = c("yield","sigma")
levels(sdf$sigma) = c("field","epsilon")
ggplot(sdf, aes(x=yield,color=sigma)) + geom_density() +xlim(0,20)
```

We see that the error SD can be localized much more than the field SD.
We can also look at the field effects:

```{r}
#| label: irristanfld
sdf = stack(draws_df[,startsWith(colnames(draws_df),"fld")])
colnames(sdf) = c("yield","fld")
levels(sdf$fld) = 1:8
ggplot(sdf, aes(x=yield,color=fld)) + geom_density() + xlim(-25,25)
```

We are looking at the differences from the overall mean. We see that all eight field distributions clearly overlap zero. There
is a distinction between the first four and the second four fields.
We can also look at the "fixed" effects:

```{r}
#| label: irristanfixed
sdf = stack(draws_df[,c("ir2","ir3","ir4","va2")])
colnames(sdf) = c("yield","fixed")
levels(sdf$fixed) = c("ir2","ir3","ir4","va2")
ggplot(sdf, aes(x=yield,color=fixed)) + geom_density() + xlim(-15,15)
```

We are looking at the differences from the reference level. We see that all four distributions clearly overlap zero
although we are able to locate the difference between the varieties more precisely than the difference between the fields.


# BRMS

[BRMS](https://paul-buerkner.github.io/brms/) stands for Bayesian Regression Models with STAN. It provides
a convenient wrapper to STAN functionality.

Fitting the model is very similar to `lmer` as seen above:

```{r}
#| label: brmfit
#| cache: true
suppressMessages(bmod <- brm(yield ~ irrigation + variety + (1|field), 
                             irrigation, iter=10000, cores=4, backend = "cmdstanr"))
```

We get some warnings but not as severe as seen with our STAN fit
above. We can obtain some posterior densities and diagnostics with:

```{r}
#| label: irribrmsdiag
plot(bmod, variable = "^s", regex=TRUE)
```

We have chosen only the random effect hyperparameters since this is
where problems will appear first. Looks OK.

We can look at the STAN code that `brms` used with:

```{r}
stancode(bmod)
```

We see that `brms` is using student t distributions with 3 degrees of
freedom for the priors. For the two error SDs, this will be truncated at
zero to form half-t distributions. You can get a more explicit description
of the priors with `prior_summary(bmod)`. These are qualitatively similar to the
half-normal and the PC prior used in the INLA fit. 

We examine the fit:

```{r}
summary(bmod)
```

The posterior mean for the field SD is more comparable to the mixed model
and INLA values seen earlier and smaller than the STAN fit. This can be
ascribed to the more informative prior used for the BRMS fit.

# MGCV

It is possible to fit some GLMMs within the GAM framework of the `mgcv`
package. An explanation of this can be found in this 
[blog](https://fromthebottomoftheheap.net/2021/02/02/random-effects-in-gams/)

The `field` term must be a factor for this to work:

```{r}
gmod = gam(yield ~ irrigation + variety + s(field,bs="re"), 
           data=irrigation, method="REML")
```

and look at the summary output:

```{r}
summary(gmod)
```

We get the fixed effect estimates.
We also get a test on the random effect (as described in this [article](https://doi.org/10.1093/biomet/ast038). The hypothesis of no variation
between the fields is rejected.

We can get an estimate of the operator and error SD:

```{r}
gam.vcomp(gmod)
```

which is the same as the REML estimate from `lmer` earlier.

The random effect estimates for the fields can be found with:

```{r}
coef(gmod)
```


# GINLA

In [Wood (2019)](https://doi.org/10.1093/biomet/asz044), a
simplified version of INLA is proposed. The first
construct the GAM model without fitting and then use
the `ginla()` function to perform the computation.

```{r}
gmod = gam(yield ~ irrigation + variety + s(field,bs="re"), 
           data=irrigation, fit = FALSE)
gimod = ginla(gmod)
```

We get the posterior density for the intercept as:

```{r}
#| label: irriginlaint
plot(gimod$beta[1,],gimod$density[1,],type="l",xlab="yield",ylab="density")
```

and for the treatment effects as:

```{r}
#| label: irriginlateff
xmat = t(gimod$beta[2:5,])
ymat = t(gimod$density[2:5,])
matplot(xmat, ymat,type="l",xlab="yield",ylab="density")
legend("right",c("i2","i3","i4","v2"),col=1:4,lty=1:4)
```

```{r}
#| label: irriginlareff
xmat = t(gimod$beta[6:13,])
ymat = t(gimod$density[6:13,])
matplot(xmat, ymat,type="l",xlab="yield",ylab="density")
legend("right",paste0("field",1:8),col=1:8,lty=1:8)
```

It is not straightforward to obtain the posterior densities of
the hyperparameters. 

# Discussion

See the [Discussion of the single random effect model](pulp.md#Discussion) for
general comments. Given that the fixed effects are not significant here,
this example is not so different from the single random effect example. This
provides an illustration of why we need to pay attention to the priors. In
the `pulp` example, the default priors resulted in unbelievable results from
INLA and we were prompted to consider alternatives. In this example, the 
default INLA priors produce output that looked passable and, if we were
feeling lazy, we might have skipped a look at the alternatives. In this case,
they do not make much difference. Contrast this with the default STAN priors
used - the output looks reasonable but the answers are somewhat different. Had
we not been trying these other analyses, we would not be aware of this. The
minimal analyst might have stopped there. But BRMS uses more informative priors
and produces results closer to the other methods.

STAN/BRMS put more weight on low values of the random effects SDs whereas the INLA posteriors are clearly bounded away from zero. We saw a similar effect in
the `pulp` example. Although we have not matched up the priors exactly, there
does appear to be some structural difference.

Much of the worry above centers on the random effect SDs. The fixed effects
seem quite robust to these concerns. If we only care about these, GINLA is
giving us what we need with the minimum amount of effort (we would not even
need to install any packages beyond the default distribution of R, though
this is an historic advantage for `mgcv`).


# Package version info


```{r}
sessionInfo()
```







