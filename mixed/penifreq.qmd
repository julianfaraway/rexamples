---
title: Randomized Block Design fit using Frequentist Methods
author: "[Julian Faraway](https://julianfaraway.github.io/)"
date: "`r format(Sys.time(), '%d %B %Y')`"
format: 
  gfm:
    toc: true
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment=NA, 
                      echo = TRUE,
                      fig.path="figs/",
                      dev = 'svglite',  
                      fig.ext = ".svg",
                      warning=FALSE, 
                      message=FALSE)
knitr::opts_knit$set(global.par = TRUE)
```

```{r graphopts, include=FALSE}
par(mgp=c(1.5,0.5,0), mar=c(3.1,3.1,0.1,0), pch=20)
ggplot2::theme_set(ggplot2::theme_bw())
```

See the [introduction](../index.md) for an overview. 

See a [mostly Bayesian analysis](penicillin.md) analysis of the same
data. 

This example is discussed in more detail in my book
[Extending the Linear Model with R](https://julianfaraway.github.io/faraway/ELM/)



```{r}
library(faraway)
library(ggplot2)
library(knitr)
```

# Data

Load in and plot the data:

```{r peni}
data(penicillin, package="faraway")
summary(penicillin)
ggplot(penicillin,aes(x=blend,y=yield,group=treat,linetype=treat))+geom_line()
ggplot(penicillin,aes(x=treat,y=yield,group=blend,linetype=blend))+geom_line()
```

The production of penicillin uses a raw material, corn steep liquor, which is quite variable and can only be made in blends sufficient for four runs. There are four processes, A, B, C and D, for the production. See `help(penicillin)` for more information about the data.

In this example, the treatments are the four processes. These are the specific
four processes of interest that we wish to compare. The five blends are five
among many blends that would be randomly created during production. We are
not interested in these five specific blends but are interested in how
the blends vary. An interaction between blends and treatments would complicate
matters. But (a) there is no reason to expect this exists and (b) with only one
replicate per treatment and blend combination, it is difficult to check for
an interaction.

The plots show no outliers, no skewness, no obviously unequal variances and 
no clear evidence of interaction. Let's proceed.

# Questions

1. Is there a difference between treatments? If so, what?
2. Is there variation between the blends? What is the extent of this variation?

# Linear Mixed Model

Consider the model: 
$$
y_{ijk} = \mu + \tau_i + v_j + \epsilon_{ijk}
$$

where the $\mu$ and$\tau_i$ are fixed effects and the error
$\epsilon_{ijk}$ is independent and
identically distributed $N(0,\sigma^2)$. The $v_j$ are random
effects and are independent and
identically distributed $N(0,\sigma^2_v)$. 

# LME4

```{r}
library(lme4)
```

We fit the model
using REML: 
```{r}
mmod <- lmer(yield ~ treat + (1|blend), penicillin)
summary(mmod, cor = FALSE)
```
We get fixed effect estimates for the treatments but an
estimated blend SD. We can get random effect estimates:
```{r}
ranef(mmod)$blend
```
We can test for a difference of the fixed effects with:
```{r}
anova(mmod)
```
No p-value is supplied because there is some doubt in general over
the validity of the null F-distribution. In this specific example, with
a simple balanced design, it can be shown that the null F is correct. (For
other unbalanced or more complex designs, it would not be correct, hence
the caution about testing in `lme4`). Even if we use an F-distribution,
it's not obvious what degrees of freedom to use for the denominator. The
usual heuristics about counting parameters do not apply because it's not
clear how to account for the random effects. There are various adjustment
methods for computing the degrees of freedom.

We can use the Kenward-Roger method with:
```{r}
library(pbkrtest)
amod <- lmer(yield ~ treat + (1|blend), penicillin, REML=FALSE)
nmod <- lmer(yield ~ 1 + (1|blend), penicillin, REML=FALSE)
KRmodcomp(amod, nmod)
```
There is no evidence of a difference between the treatments. 

Testing the random effects is more challenging.
We can test the hypothesis $H_0: \sigma^2_v = 0$ using a parametric 
bootstrap method:
```{r penifreqbootblend, cache=TRUE}
rmod <- lmer(yield ~ treat + (1|blend), penicillin)
nlmod <- lm(yield ~ treat, penicillin)
as.numeric(2*(logLik(rmod)-logLik(nlmod,REML=TRUE)))
lrstatf <- numeric(1000)
for(i in 1:1000){
   ryield <-  unlist(simulate(nlmod))
   nlmodr <- lm(ryield ~ treat, penicillin)
   rmodr <- lmer(ryield ~ treat + (1|blend), penicillin)
   lrstatf[i] <- 2*(logLik(rmodr)-logLik(nlmodr,REML=TRUE))
  }
mean(lrstatf > 2.7629)
```
The result falls just below the 5\% level for significance. Because
of resampling variability, we should repeat with more bootstrap samples.

We can also test for variation in the random effects using the
[RLRsim](https://github.com/fabian-s/RLRsim) package:

```{r}
library(RLRsim)
exactRLRT(mmod)
```

Again we get a marginally significant result.

The [emmeans](https://rvlenth.github.io/emmeans/) package computes estimated
marginal means. We can use it to compute the marginal treatment effects
along with confidence intervals"

```{r}
library(emmeans)
emmeans(mmod, specs="treat")
```

The difficult issue is the calculation of the appropriate degrees of
freedom for the computation of the intervals. We see that `emmeans` is
using the Kenward-Roger method.

We can also do some pairwise comparisons.

```{r}
rem = emmeans(mmod, pairwise ~ treat)
summary(rem$contrasts,infer=TRUE)
```

There are no significant pairwise differences.


# NLME

See the discussion for the [single random effect example](pulpfreq.md#NLME)
for some introduction.

```{r}
library(nlme)
```

The syntax is different with the random effect specified as a separate term:

```{r}
nlmod = lme(yield ~ treat, penicillin, ~ 1 | blend)
summary(nlmod)
```

The estimates and standard errors are the same for the corresponding
`lme4` output. `nlme` is less inhibited about testing and reports
p-values for the fixed effects (although these are not particularly useful).

`nlme` is also happy to test the fixed effects with an F-test:

```{r}
anova(nlmod)
```

The result is the same as Kenward-Roger result reported earlier. 

The `lme` output also works with `RLRsim` package demonstrated earlier.

```{r}
exactRLRT(nlmod)
```

Random effects are the same as in `lme4`

```{r}
random.effects(nlmod)
```

We can also do some estimated marginal means with `emmeans`: 

```{r}
emmeans(nlmod, specs="treat")
```

The default method for computing the degrees of freedom is "containment"
which is very conservative (underestimates the df - 4 is very low here). We can
specify the more realistic Satterthwaite method:

```{r}
emmeans(nlmod, specs="treat", mode = "satterthwaite")
```

We can also do some pairwise comparisons

```{r}
rem = emmeans(nlmod, pairwise ~ treat, mode = "satterthwaite")
summary(rem$contrasts,infer=TRUE)
```

We can take a GLS approach to modeling the random part of the model
as used in [single random effect example](pulpfreq.md):

```{r}
gmod = gls(yield ~ treat,
           data=penicillin,
           correlation = corCompSymm(form = ~ 1|blend))
summary(gmod)
```

The fixed effect parts are the same although the F-test:

```{r}
anova(gmod)
```

comes out somewhat differently (wrong) due to a different computation of the
residuals degrees of freedom.

The `emmeans` package also works with `gls` models:

```{r}
emmeans(gmod, specs="treat", mode = "satterthwaite")
```


# MMRM

See the discussion for the [single random effect example](pulpfreq.md#MMRM)
for some introduction.

```{r}
library(mmrm)
```

As with the `pulp` example, we need to distinguish
between the different replicates for a given level
of blend. We don't need to create a visit factor
as in the previous example as `treat` serves the
same purpose:

```{r}
mmmod = mmrm(yield ~ treat + cs(treat|blend), penicillin)
summary(mmmod)
```

Fixed effect estimates are the same as in the `gls()` fit but notice
that the degrees of freedom have been adjusted down to 12dfs (as in
the Kenward-Roger adjustment for the `lme4` fit)

The random component is expressed as a covariance matrix. 
The SD down the diagonal will give the estimated error variance
from previous models:

```{r}
cm = mmmod$cov
sqrt(cm[1,1])
```

We can compute the correlation as:

```{r}
cm[1,2]/cm[1,1]
```

We see this is identical with the `gls()` fit as we would expect. 

Performing an F-test on the fixed effects is more complicated
than one might like (and the documentation is lacking in this respect).
One must specify a *contrast matrix* of a form that picks out the
combinations of the parameters that the null hypothesis sets to zero.
(Tip: it's not one of those contrasts where they have to sum to zero -
that's not a rule that is universally obeyed)
In this example, we are testing $\beta_2=\beta_3=\beta_4=0$ so
we construct a matrix $C$ such that $C\beta=0$. For our test, we
want:

```{r}
cm = matrix(0,3,4)
cm[1,2] = cm[2,3] = cm[3,4] = 1
cm
```

and then perform the test:

```{r}
df_md(mmmod, cm)
```

We get the same result as seen before. There are options on computing
the denominator degrees of freedom for the tests and confidence intervals.

An easier way to do the test is to install the `car` package which has
an extension for `mmrm` models:

```{r}
library(car)
Anova(mmmod)
```

We need not concern ourselves with the arcana of whether this is a Type X test
as there is only one term.

The `emmeans` package will also deal with `mmrm` models and produce
some confidence intervals for the marginal means:

```{r}
emmeans(mmod, specs="treat")
```

and do some pairwise comparisons:

```{r}
rem = emmeans(mmmod, pairwise ~ treat)
summary(rem$contrasts,infer=TRUE)
```

None of the pairwise comparisons are significant


# GLMMTMB

See the discussion for the [single random effect example](pulpfreq.md#GLMMTMB)
for some introduction.

```{r}
library(glmmTMB)
```

The default fit uses ML (not REML)

```{r}
gtmod <- glmmTMB(yield ~ treat + (1|blend), penicillin)
summary(gtmod)
```

This is identical with the `lme4` fit using ML.

We can use the `car` package to test the treatment effects:

```{r}
Anova(gtmod)
```

We get a chi-squared test which is less efficient than the F-test. We
could do the simulation-based testing as for `lme4` if we wanted the F-test.

The `emmeans` package also plays nice with `glmmTMB` models:

```{r}
emmeans(gtmod, specs="treat")
```

The results are not the same as for the `mmrm` because a different
degrees of freedom has been used.

```{r}
rem = emmeans(gtmod, pairwise ~ treat)
summary(rem$contrasts,infer=TRUE)
```

Again there are some differences due the degrees of freedom calculation.

# Discussion

The main difference between the packages lies in the inference. This
is unsurprising since it is a complex issue. `lme4` and `glmmTMB` subcontract
the inference to other packages. The implementation in `mmrm` is not
straightforward although using the `car` package makes it easier. `nlme`
is less inhibited giving the right answer in this case (but not for `gls`). 
Unfortunately, the uninhibited answer will not be correct in more complex
examples.

The `emmeans` package works with all four fitting packages although the
results are not all the same mainly due to the degrees of freedom
adjustment issue.



# Package version info


```{r}
sessionInfo()
```