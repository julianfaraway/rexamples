---
title: Multilevel Design
author: "[Julian Faraway](https://julianfaraway.github.io/)"
date: "`r format(Sys.time(), '%d %B %Y')`"
format: 
  gfm:
    toc: true
---

```{r}
#| label: global_options
#| include: false
knitr::opts_chunk$set(comment=NA, 
                      echo = TRUE,
                      fig.path="figs/",
                      dev = 'svglite',  
                      fig.ext = ".svg",
                      warning=FALSE, 
                      message=FALSE)
knitr::opts_knit$set(global.par = TRUE)
```

```{r}
#| label: graphopts
#| include: false
par(mgp=c(1.5,0.5,0), mar=c(3.1,3.1,0.1,0), pch=20)
ggplot2::theme_set(ggplot2::theme_bw())
```

See the [introduction](index.md) for an overview. 

This example is discussed in more detail in my book
[Extending the Linear Model with R](https://julianfaraway.github.io/faraway/ELM/)

Required libraries:

```{r}
library(faraway)
library(ggplot2)
library(lme4)
library(pbkrtest)
library(RLRsim)
library(INLA)
library(knitr)
library(cmdstanr)
register_knitr_engine(override = FALSE)
library(brms)
library(mgcv)
```

# Data

*Multilevel* models is a term used for models for data with hierarchical
structure. The term is most commonly used in the social sciences.
We can use the methodology we have already developed to fit some of these models.

We take as our example some data from the Junior School Project collected
from primary (U.S. term is elementary) schools in inner London. 
We math test score result from year two as the response
and try to model this as a function of gender, social class and the Raven's test
score from the first year which might be taken as a measure of ability when entering
the school. We subset the data to ignore the math scores from the first two years,
we centre the Raven score and create a combined class-by-school label:


```{r}
data(jsp, package="faraway")
jspr <- jsp[jsp$year==2,]
jspr$craven <- jspr$raven-mean(jspr$raven)
jspr$classch <- paste(jspr$school,jspr$class,sep=".")
```

We can plot the data

```{r}
#| label: jspplot
ggplot(jspr, aes(x=raven, y=math))+xlab("Raven Score")+ylab("Math Score")+geom_point(position = position_jitter())
ggplot(jspr, aes(x=social, y=math))+xlab("Social Class")+ylab("Math Score")+geom_boxplot()
```

# Mixed Effect Model

Although the data supports a more complex model, we simplify to having the centred Raven score and
the social class as fixed effects and the school and class nested within school as
random effects. 
See [Extending the Linear Model with R](https://julianfaraway.github.io/faraway/ELM/),

```{r}
mmod <- lmer(math ~ craven + social+(1|school)+(1|school:class),jspr)
faraway::sumary(mmod)
```

We can see the math score is strongly related to
the entering Raven score. We see that the math score tends to be lower as social class goes down. 
We also see the most substantial variation at the individual level with smaller
amounts of variation at the school and class level.

We test the random effects:

```{r}
mmodc <- lmer(math ~ craven + social+(1|school:class),jspr)
mmods <- lmer(math ~ craven + social+(1|school),jspr)
exactRLRT(mmodc, mmod, mmods)
exactRLRT(mmods, mmod, mmodc)
```

The first test is for the class effect which just fails to meet the 5% significance level.
The second test is for the school effect and shows strong evidence of differences between
schools.


We can test the social fixed effect:

```{r}
mmodm <- lmer(math ~ craven + (1|school)+(1|school:class),jspr)
KRmodcomp(mmod, mmodm)
```

We see the social effect is significant.

We can compute confidence intervals for the parameters:

```{r}
#| label: jspconfint
#| cache: true
confint(mmod, method="boot")
```

The lower end of the class confidence interval is zero while the school random effect
is clearly larger. This is consistent with the earlier tests.


# INLA

Integrated nested Laplace approximation is a method of Bayesian computation
which uses approximation rather than simulation. More can be found
on this topic in [Bayesian Regression Modeling with INLA](http://julianfaraway.github.io/brinla/) and the 
[chapter on GLMMs](https://julianfaraway.github.io/brinlabook/chaglmm.html)

Use the most recent computational methodology:


```{r}
inla.setOption(inla.mode="experimental")
inla.setOption("short.summary",TRUE)
```

```{r}
#| label: jspinladef
#| cache: true
formula <- math ~ social+craven + f(school, model="iid") + f(classch, model="iid")
result <- inla(formula, family="gaussian", data=jspr)
summary(result)
```

As usual, the default priors result in precisions for the random effects which are unbelievably large and we need to change the default prior.

## Informative Gamma priors on the precisions

Now try more informative gamma priors for the random effect precisions. Define it so
the mean value of gamma prior is set to the inverse of the variance of
the residuals of the fixed-effects only model. We expect the error
variances to be lower than this variance so this is an overestimate.
The variance of the gamma prior (for the precision) is controlled by
the `apar` shape parameter.

```{r}
#| label: jspinlaig
#| cache: true
apar <- 0.5
lmod <- lm(math ~ social+craven, jspr)
bpar <- apar*var(residuals(lmod))
lgprior <- list(prec = list(prior="loggamma", param = c(apar,bpar)))
formula = math ~ social+craven+f(school, model="iid", hyper = lgprior)+f(classch, model="iid", hyper = lgprior)
result <- inla(formula, family="gaussian", data=jspr)
summary(result)
```

Results are more credible.

Compute the transforms to an SD scale for the random effect terms. Make a table of summary statistics for the posteriors:

```{r}
#| label: sumstats
sigmasch <- inla.tmarginal(function(x) 1/sqrt(exp(x)),result$internal.marginals.hyperpar[[2]])
sigmacla <- inla.tmarginal(function(x) 1/sqrt(exp(x)),result$internal.marginals.hyperpar[[3]])
sigmaepsilon <- inla.tmarginal(function(x) 1/sqrt(exp(x)),result$internal.marginals.hyperpar[[1]])
restab=sapply(result$marginals.fixed, function(x) inla.zmarginal(x,silent=TRUE))
restab=cbind(restab, inla.zmarginal(sigmasch,silent=TRUE))
restab=cbind(restab, inla.zmarginal(sigmacla,silent=TRUE))
restab=cbind(restab, inla.zmarginal(sigmaepsilon,silent=TRUE))
colnames(restab) = c("mu",result$names.fixed[2:10],"school SD","class SD","epsilon")
data.frame(restab) |> kable()
```

Also construct a plot the SD posteriors:

```{r}
#| label: jsppostsd
ddf <- data.frame(rbind(sigmasch,sigmacla,sigmaepsilon),errterm=gl(3,nrow(sigmasch),labels = c("school","class","epsilon")))
ggplot(ddf, aes(x,y, linetype=errterm))+geom_line()+xlab("math")+ylab("density")
```

Posteriors look OK although no weight given to smaller values.

## Penalized Complexity Prior

In [Simpson et al (2015)](http://arxiv.org/abs/1403.4630v3), penalized complexity priors are proposed. This
requires that we specify a scaling for the SDs of the random effects. We use the SD of the residuals
of the fixed effects only model (what might be called the base model in the paper) to provide this scaling.

```{r}
#| label: jspinlapc
#| cache: true
lmod <- lm(math ~ craven + social, jspr)
sdres <- sd(residuals(lmod))
pcprior <- list(prec = list(prior="pc.prec", param = c(3*sdres,0.01)))
formula = math ~ social+craven+f(school, model="iid", hyper = pcprior)+f(classch, model="iid", hyper = pcprior)
result <- inla(formula, family="gaussian", data=jspr)
summary(result)
```

Compute the summaries as before:

```{r}
#| ref.label: sumstats
```

Make the plots:

```{r}
#| label: jsppostsdpc
ddf <- data.frame(rbind(sigmasch,sigmacla,sigmaepsilon),errterm=gl(3,nrow(sigmasch),labels = c("school","class","epsilon")))
ggplot(ddf, aes(x,y, linetype=errterm))+geom_line()+xlab("math")+ylab("density")
```


Posteriors put more weight on lower values compared to gamma prior.

# STAN

[STAN](https://mc-stan.org/) performs Bayesian inference using
MCMC. I use `cmdstanr` to access Stan from R.

You see below the Stan code to fit our model. Rmarkdown allows the use
of Stan chunks (elsewhere I have R chunks). The chunk header looks
like this. 

STAN chunk will be compiled to 'mod'. Chunk header is:
```
cmdstan, output.var="mod", override = FALSE
```

```{cmdstan, output.var="mod", override = FALSE}
data {
     int<lower=0> Nobs;
     int<lower=0> Npreds;
     int<lower=0> Nlev1;
     int<lower=0> Nlev2;
     array[Nobs] real y;
     matrix[Nobs,Npreds] x;
     array[Nobs] int<lower=1,upper=Nlev1> levind1;
     array[Nobs] int<lower=1,upper=Nlev2> levind2;
     real<lower=0> sdscal;
}
parameters {
           vector[Npreds] beta;
           real<lower=0> sigmalev1;
           real<lower=0> sigmalev2;
           real<lower=0> sigmaeps;

           vector[Nlev1] eta1;
           vector[Nlev2] eta2;
}
transformed parameters {
  vector[Nlev1] ran1;
  vector[Nlev2] ran2;
  vector[Nobs] yhat;

  ran1  = sigmalev1 * eta1;
  ran2  = sigmalev2 * eta2;

  for (i in 1:Nobs)
    yhat[i] = x[i]*beta+ran1[levind1[i]]+ran2[levind2[i]];

}
model {
  eta1 ~ normal(0, 1);
  eta2 ~ normal(0, 1);
  sigmalev1 ~ cauchy(0, 2.5*sdscal);
  sigmalev2 ~ cauchy(0, 2.5*sdscal);
  sigmaeps ~ cauchy(0, 2.5*sdscal);
  y ~ normal(yhat, sigmaeps);
}
```

We have used uninformative priors
for the treatment effects but slightly informative half-cauchy priors
for the three variances. All the fixed effects have been collected
into a single design matrix.  The school and class variables need to
be renumbered into consecutive positive integers. Somewhat
inconvenient since the schools are numbered up to 50 but have no data
for two schools so only 48 schools are actually used.

```{r}
lmod <- lm(math ~ craven + social, jspr)
sdscal <- sd(residuals(lmod))
Xmatrix <- model.matrix( ~ craven + social, jspr)
jspr$school <- factor(jspr$school)
jspr$classch <- factor(paste(jspr$school,jspr$class,sep="."))
jspdat <- list(Nobs=nrow(jspr),
               Npreds=ncol(Xmatrix),
               Nlev1=length(unique(jspr$school)),
               Nlev2=length(unique(jspr$classch)),
               y=jspr$math,
               x=Xmatrix,
               levind1=as.numeric(jspr$school),
               levind2=as.numeric(jspr$classch),
               sdscal=sdscal)
```

Do the MCMC sampling:

```{r}
fit <- mod$sample(
  data = jspdat, 
  seed = 123, 
  chains = 4, 
  parallel_chains = 4,
  refresh = 0 # print no updates
)
```

## Diagnostics

Extract the draws into a convenient dataframe format:

```{r}
draws_df <- fit$draws(format = "df")
```


For the School SD:

```{r}
#| label: jspsigmalev1
ggplot(draws_df,
       aes(x=.iteration,y=sigmalev1,color=factor(.chain))) + geom_line() +
  labs(color = 'Chain', x="Iteration")
```

For the class SD

```{r}
#| label: jspsigmalev2
ggplot(draws_df,
       aes(x=.iteration,y=sigmalev2,color=factor(.chain))) + geom_line() +
  labs(color = 'Chain', x="Iteration")
```

All these are satisfactory.

## Output Summary

Display the parameters of interest:

```{r}
fit$summary(c("beta","sigmalev1","sigmalev2","sigmaeps"))
```

Remember that the beta correspond to the following parameters:

```{r}
colnames(Xmatrix)
```

The results are comparable to the REML fit. The effective sample sizes are sufficient.

## Posterior Distributions

We can use extract to get at various components of the STAN fit. First consider the SDs for random components:

```{r}
#| label: jsppostsig
sdf = stack(draws_df[,c("sigmaeps","sigmalev1","sigmalev2")])
colnames(sdf) = c("math","SD")
levels(sdf$SD) = c("individual","school","class")
ggplot(sdf, aes(x=math,color=SD)) + geom_density() 
```

As usual the error SD distribution is a more concentrated. The school SD is more diffuse and smaller whereas the class SD is smaller
still. Now the treatement effects, considering the social class parameters first:


```{r}
#| label: jspbetapost
sdf = stack(draws_df[,4:11])
colnames(sdf) = c("math","social")
levels(sdf$social) = 2:9
ggplot(sdf, aes(x=math,color=social)) + geom_density() 
```

Now just the raven score parameter:

```{r}
#| label: jspcravenpost
ggplot(draws_df, aes(x=.data[["beta[2]"]])) + geom_density() + xlab("Math per Raven")
```

Now for the schools:

```{r}
#| label: jspschoolspost
sdf = stack(draws_df[,startsWith(colnames(draws_df),"ran1")])
colnames(sdf) = c("math","school")
levels(sdf$school) = 1:48
ggplot(sdf, aes(x=math,group=school)) + geom_density() 
```

We can see the variation between schools. A league table might be used to rank the schools but the
high overlap in these distributions show that such a ranking should not be interpreted too seriously.

# BRMS

[BRMS](https://paul-buerkner.github.io/brms/) stands for Bayesian Regression Models with STAN. It provides a convenient wrapper to STAN functionality. We specify the model as in `lmer()` above.
I have used more than the standard number of iterations because this reduces some problems
and does not cost much computationally.


```{r}
#| label: jspbrmfit
#| cache: true
bmod <- brm(math ~ craven + social+(1|school)+(1|school:class),
            data=jspr,iter=10000, cores=4, refresh=0,
            backend = "cmdstanr")
```

We get some minor warnings. We can obtain some posterior densities and diagnostics with:

```{r}
#| label: jspbrmsdiag
plot(bmod, variable = "^s", regex=TRUE)
```

We have chosen only the random effect hyperparameters since this is
where problems will appear first. Looks OK. We can see some weight
is given to values of the class effect SD close to zero.

We can look at the STAN code that `brms` used with:

```{r}
stancode(bmod)
```

We see that `brms` is using student t distributions with 3 degrees of
freedom for the priors. For the three error SDs, this will be truncated at
zero to form half-t distributions. You can get a more explicit description
of the priors with `prior_summary(bmod)`. These are qualitatively similar to the
the PC prior used in the INLA fit. 

We examine the fit:

```{r}
summary(bmod)
```

The results are consistent with those seen previously.

# MGCV

It is possible to fit some GLMMs within the GAM framework of the `mgcv`
package. An explanation of this can be found in this 
[blog](https://fromthebottomoftheheap.net/2021/02/02/random-effects-in-gams/)


```{r}
gmod = gam(math ~ craven + social+s(school,bs="re")+s(classch,bs="re"),data=jspr, method="REML")
```

and look at the summary output:

```{r}
summary(gmod)
```

We get the fixed effect estimates.
We also get tests on the random effects (as described in this [article](https://doi.org/10.1093/biomet/ast038). The hypothesis of no variation
is rejected for the school but not for the class. This is consistent
with earlier findings.

We can get an estimate of the operator and error SD:

```{r}
gam.vcomp(gmod)
```

The point estimates are the same as the REML estimates from `lmer` earlier.
The confidence intervals are different. A bootstrap method was used for
the `lmer` fit whereas `gam` is using an asymptotic approximation resulting
in substantially different results. Given the problems of parameters on
the boundary present in this example, the bootstrap results appear more
trustworthy.

The fixed and random effect estimates can be found with:

```{r}
coef(gmod)
```

# GINLA

In [Wood (2019)](https://doi.org/10.1093/biomet/asz044), a
simplified version of INLA is proposed. The first
construct the GAM model without fitting and then use
the `ginla()` function to perform the computation.

```{r}
gmod = gam(math ~ craven + social+s(school,bs="re")+s(classch,bs="re"),
           data=jspr, fit = FALSE)
gimod = ginla(gmod)
```

We get the posterior density for the intercept as:

```{r}
#| label: jspginlaint
plot(gimod$beta[1,],gimod$density[1,],type="l",xlab="math",ylab="density")
```

We get the posterior density for the raven effect as:

```{r}
#| label: jspginlaraven
plot(gimod$beta[2,],gimod$density[2,],type="l",xlab="math per raven",ylab="density")
```

and for the social effects as:

```{r}
#| label: jspginlalsoc
xmat = t(gimod$beta[3:10,])
ymat = t(gimod$density[3:10,])
matplot(xmat, ymat,type="l",xlab="math",ylab="density")
legend("left",paste0("social",2:9),col=1:8,lty=1:8)
```

We can see some overlap between the effects, but strong evidence of a negative outcome
relative to social class 1 for some classes.


It is not straightforward to obtain the posterior densities of
the hyperparameters. 

# Discussion

See the [Discussion of the single random effect model](pulp.md#Discussion) for
general comments. 

- As with the previous analyses, sometimes the INLA posteriors for the hyperparameters have densities
which do not give weight to close-to-zero values where other analyses suggest this might be reasonable.

- There is relatively little disagreement between the methods and much similarity.

- There were no major computational issue with the analyses (in contrast with some of
the other examples)

- The `mgcv` analyses took a little longer than previous analyses because the sample size
is larger (but still were quite fast). 

- August 2024: rerunning the code with the latest versions of packages produces much the
same results except for the PC prior INLA fit. The current fit is more inline with
the other Bayesian fits. In contrast the INLA with gamma prior results
was similar to the previous year.

# Package version info

```{r}
sessionInfo()
```



